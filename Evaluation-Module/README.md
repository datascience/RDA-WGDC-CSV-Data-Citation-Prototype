# Evaluation Approaxch (new)

The evaluation is CSV file centric. Therefore all operations are based on the CSV files. In order to compare git Git (Evaluation System ES) with the Database approach (Prorotype System PS), we will measure the insert time and the re-execution time of each CSV file and the corresponding queries as well as the storage demand for the complete repository.

## Steps

### Phase 1: Data creation (generate several CSV data sets)

The first phase is responsible for generating the data sets. A data set is a randomized CSV file, consisting of several columns (where at least one column is unqiue and serves as primary key). Time and storage demand are measured for each data set individually and stored in a database for later evaluation.

1. Create a random CSV file based on the distribution parameters length, columns and amount of records.
2. Upload the CSV file into the PS (create a base table) (measure time and storage space).
3. Rename the CSV file with the PID generated by the PS to enable matching between approaches
4. Add the renamed CSV file into the ES (git add) (measure time and storage space)

### Phase 2: Simulate data modification (repeat for each data set )

The second phase is responsible for simulating data modification, which includes adding, altering or removing records (i.e. rows) from the CSV files. This creates a history of the data set in both systems. The PS creates this history in the database, the ES creates this history in the Git repository via commits. We will wait one second between each new operation but assign the same timestamp in both systems. Time and storage demand are measured for each data set individually and stored in a database for later evaluation.

5. Generate random operations (INSERT, UPDATE or DELETE ) and apply them on the CSV file directly. These operpations are randomly selected based on a distribution. INSERT adds a new record into the CSV file (a random row), UPDATE replaces a row but preserves the primary key, DELETE deletes a record. 
6. Upload the changed file into the PS, which will detect the changes and reflect them in the database. (measure space and time)
7. Commit the changed file into the ES (measure space and time)

### Phase 3: Simulate Queries (Re-execute, repeat for each data set)

The third phase is responsible for generating subsets by using the query mechanism. Here we only use SELECT statements for both approaches. Time is measured for each data set individually and stored in a database for later evaluation.

8. Generate a random SELECT query based on the distribution of simple, average and complex queries. This will create a new record in the Query Store, which creates a subset by randomly selecting columns and adds filter and sorting criteria.
9. Compute a random timestamp between the insertion date and the last change of the data set, with the accuracy of one second. This timestamp serves as the re-execution time.
10. Re-execute the query in the PS and export the result as CSV file (measure time)
11. Re-execute the query in the ES and export the result as CSV file (measure time)



# Evaluation Alt

## Steps

Several steps are necessary for generating the files, creating the database schemata and performing the evaluation.

1. Make sure the system is initialised:  mysql -u querystoreuser -pquery2014 < Minimal-CSV-CitationBaseSystem.sql
2. Create the test data set using GenerateTestDataMain.java in Package TestDataGenerator
3. Upload the file into the System with DataPreparationMain in Package Data Preparation
4. Create queries with the QueryGeneratorMain

# Neue Idee

Verwende MySQL auch für den Git Ansatz. Zuerst wird das Testdatenset erzeugt und dann wird dieses in den Data Citation Ansatz hochgeladen.
Danach werden Queries erzeugt. Die Queries für den DC Ansatz werden wir gehabt versioniert gegen den Data Store gespielt. Für den Git Ansatz wird die vereinfachte Query bzw. deren Result Set jedesmal rausgespielt und als CSV exportiert. Die Abfrage erfolgt dann mittels csvkit query auf dem CSV file.

1. Erzeuge testdaten
2. Lade testdatenset in MySQL
4. Erzeuge queries.
5. Jede Query hat einen Execution Timestamp
6. Führe queries aus.
6.1 INSERT, UPDATE und DELETE werden auf der Original Tabelle im SQL System durchgeführt. Nach jeder Operation erfolgt ein CSV export und Git commit. Speichere für jede Query die verwendeten Columns, die Tabelle sowie die Where Klausel mit.

6.2 SELECT wird auf beiden Systemen durchgeführt und die Zeit wird gemessen. Als Refernzzeit gilt bei beiden die selbe, die dann entweder für das erweiterte SQL Statement oder für Git SQL2CSV verwendet wird zum auschecken.

# Nach Gespräch mit Andi:

Immer von CSV Files ausgehen, d.h. auch die Updates und Deletes werden im File durchgeführt, nicht direkt in der DB. 
Deswegen vorher 100(e) CSV Files erzeugen und dann von jedem viele Versionen anlegen. Aus jedem File wird dann anhand
der vorgegebenen Verteilung der Operationen (INSERT, UPDATE, DELETE) am File die Änderung durchgeführt und dann wird
das File wieder ins System geladen.

Spaltentypen sollten erkannt werden, also Strings, Integers und Datumswerte.

   
